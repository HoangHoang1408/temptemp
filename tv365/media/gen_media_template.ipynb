{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import g4f\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz\n",
    "from glob import glob\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g4f.Provider.GeekGpt.GeekGpt"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g4f.Provider.GeekGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pool(n, pool):\n",
    "    if n >= len(pool):\n",
    "        return pool\n",
    "    return list(np.random.choice(pool, size=n, replace=False))\n",
    "\n",
    "\n",
    "def sample_seed(n, seed):\n",
    "    return list(np.random.choice(seed, size=n, replace=False))\n",
    "\n",
    "\n",
    "TEMPLATE1 = \"\"\"Tôi đang xây dựng module tìm kiếm trong hệ thống TV trực tuyến chiếu các chương trình truyền hình. Tưởng tượng bạn là một người dùng ra lệnh cho hệ thống, hãy sinh ra các template người dùng thường nhập vào để tìm kiếm phim mà họ mong muốn.\n",
    "\n",
    "Ý định của người dùng (user_intent) có thể là:\n",
    "1) Mở trực tiếp chương trình (mo_chuong_trinh)\n",
    "2) Tìm chương trình (tim_chuong_trinh)\n",
    "\n",
    "Sau đây là một số ví dụ, hãy dựa vào các ví dụ này để xây dựng nhiều các template khác:\n",
    "{templates}\n",
    "\n",
    "\"\"\"\n",
    "TEMPLATE2 = \"\"\"Lưu ý: \n",
    "1) Các template cần đa dạng, không trùng lặp về mặt nội dung, mô tả đúng các tình huống thực tế của người dùng.\n",
    "2) Chỉ trả về các template dưới dạng json: [{'template': 'your_template', 'intent':'user_intent'}, ...] không giải thích gì thêm.\"\"\"\n",
    "\n",
    "def get_response(examples):\n",
    "    while True:\n",
    "        try:\n",
    "            templates = \"\\n\".join(examples)\n",
    "            prompt = TEMPLATE1.format(templates=templates) + TEMPLATE2\n",
    "            response = g4f.ChatCompletion.create(\n",
    "                provider=g4f.Provider.GeekGpt,\n",
    "                model=g4f.models.gpt_4,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Request failed\")\n",
    "            sleep(5)\n",
    "\n",
    "\n",
    "def extract_json(response):\n",
    "    start = response.find(\"[\")\n",
    "    end = response.find(\"]\") + 1\n",
    "    if start == -1 or end == -1:\n",
    "        return None\n",
    "    try:\n",
    "        return eval(response[start:end])\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def remove_duplicates(l, threshold=80):\n",
    "    final = []\n",
    "    for i in l:\n",
    "        can_add = True\n",
    "        for j in final:\n",
    "            score = fuzz.ratio(i[\"template\"], j[\"template\"])\n",
    "            if score >= threshold:\n",
    "                can_add = False\n",
    "                break\n",
    "        if can_add:\n",
    "            final.append(i)\n",
    "    print(\n",
    "        f\"Removed {len(l) - len(final)} similar questions from total {len(l)} questions.\"\n",
    "    )\n",
    "    return final\n",
    "\n",
    "\n",
    "def generate_questions(\n",
    "    n_rounds=20,\n",
    "    seed_path=\"./media_seed.txt\",\n",
    "    pool_path=\"./media_pool.txt\",\n",
    "    threshold=90,\n",
    "    save_steps=5,\n",
    "    n_seed=4,\n",
    "    n_pool=6,\n",
    "):\n",
    "    for i in range(n_rounds):\n",
    "        question_pool = []\n",
    "        with open(pool_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                question_pool.append(eval(line.strip())[\"template\"])\n",
    "\n",
    "        with open(seed_path, \"r\") as f:\n",
    "            seed_tasks = f.read().splitlines()\n",
    "\n",
    "        print(f\"Round {i+1}\")\n",
    "        seed = sample_seed(n_seed, seed_tasks)\n",
    "        pool = sample_pool(n_pool, question_pool)\n",
    "        response = get_response(seed + pool)\n",
    "        js = extract_json(response)\n",
    "        if js is None:\n",
    "            print(\"No json found\")\n",
    "            continue\n",
    "        with open(pool_path, \"a\") as f:\n",
    "            for q in js:\n",
    "                f.write(str(q) + \"\\n\")\n",
    "\n",
    "        if (i + 1) % save_steps == 0:\n",
    "            with open(pool_path, \"r\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "                lines = [eval(line.strip()) for line in lines]\n",
    "            lines = remove_duplicates(lines, threshold=threshold)\n",
    "\n",
    "            with open(pool_path, \"w\") as f:\n",
    "                for line in lines:\n",
    "                    f.write(str(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "New pypi version: 0.1.9.6 (current: 0.1.9.3) | pip install -U g4f\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "Round 2\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "Round 3\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "Round 4\n",
      "Round 5\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n",
      "429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions\n",
      "Request failed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     29\u001b[0m prompt \u001b[38;5;241m=\u001b[39m TEMPLATE1\u001b[38;5;241m.\u001b[39mformat(templates\u001b[38;5;241m=\u001b[39mtemplates) \u001b[38;5;241m+\u001b[39m TEMPLATE2\n\u001b[0;32m---> 30\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mg4f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg4f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProvider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeekGpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg4f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/g4f/__init__.py:101\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(model, messages, provider, stream, auth, ignored, ignore_working, ignore_stream_and_auth, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m result \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mcreate_completion(model\u001b[38;5;241m.\u001b[39mname, messages, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/g4f/Provider/GeekGpt.py:58\u001b[0m, in \u001b[0;36mGeekGpt.create_completion\u001b[0;34m(cls, model, messages, stream, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ai.fakeopen.com/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     57\u001b[0m                          headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/v1/chat/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[0;34m(n_rounds, seed_path, pool_path, threshold, save_steps, n_seed, n_pool)\u001b[0m\n\u001b[1;32m     89\u001b[0m seed \u001b[38;5;241m=\u001b[39m sample_seed(n_seed, seed_tasks)\n\u001b[1;32m     90\u001b[0m pool \u001b[38;5;241m=\u001b[39m sample_pool(n_pool, question_pool)\n\u001b[0;32m---> 91\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m js \u001b[38;5;241m=\u001b[39m extract_json(response)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m js \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_questions(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
